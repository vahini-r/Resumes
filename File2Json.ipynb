{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     ------                                   0.3/1.5 MB 7.9 MB/s eta 0:00:01\n",
      "     --------------                           0.6/1.5 MB 8.9 MB/s eta 0:00:01\n",
      "     ---------------------                    0.8/1.5 MB 6.4 MB/s eta 0:00:01\n",
      "     -----------------------------            1.1/1.5 MB 7.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 6.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 6.4 MB/s eta 0:00:00\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.4-py3-none-any.whl (98 kB)\n",
      "                                              0.0/98.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 98.2/98.2 kB ? eta 0:00:00\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "                                              0.0/302.0 kB ? eta -:--:--\n",
      "     -------------------------------------  297.0/302.0 kB 6.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 302.0/302.0 kB 6.3 MB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2023.6.3-cp311-cp311-win_amd64.whl (268 kB)\n",
      "                                              0.0/268.0 kB ? eta -:--:--\n",
      "     ------------------------------------- 268.0/268.0 kB 16.1 MB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "                                              0.0/77.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 77.1/77.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\vahin\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.4 joblib-1.3.1 nltk-3.8.1 regex-2023.6.3 tqdm-4.65.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\vahin\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script nltk.exe is installed in 'c:\\Users\\vahin\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'exceptions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpdfplumber\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdocx\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\docx.py:30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     TAGS \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39mPendingDeprecationWarning\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwarnings\u001b[39;00m \u001b[39mimport\u001b[39;00m warn\n\u001b[0;32m     33\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'exceptions'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import spacy\n",
    "import pdfplumber\n",
    "import docx\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "\n",
    "# Load spaCy's English model for entity recognition\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Token_Reduction(file_path):\n",
    "    def convert_file_to_text(file_path):\n",
    "        if file_path.endswith('.pdf'):\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                text = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    text += page.extract_text()\n",
    "                return text\n",
    "        elif file_path.endswith('.docx'):\n",
    "            doc = docx.Document(file_path)\n",
    "            paragraphs = [p.text for p in doc.paragraphs]\n",
    "            return '\\n'.join(paragraphs)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Only PDF and DOCX are supported.\")\n",
    "\n",
    "    def clean_text(text):\n",
    "        def remove_extra_white_space_and_punctuations(text):\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            punctuations_to_keep = ['. ', '@', '-', ':', '/', '+', '%', '\\\\']\n",
    "            pattern = r'[^a-zA-Z0-9\\s' + re.escape(''.join(punctuations_to_keep)) + r']+'\n",
    "            text = re.sub(pattern, '', text)\n",
    "            return text\n",
    "\n",
    "        def preserve_urls(match):\n",
    "            return match.group()\n",
    "\n",
    "        def preserve_emails(match):\n",
    "            return match.group()\n",
    "\n",
    "        def process_text(text):\n",
    "            pattern_urls = r\"(https?://[^\\s]+|<a\\s+href=['\\\"]([^'\\\"]+)['\\\"]>.*<\\/a>)\"\n",
    "            pattern_emails = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
    "\n",
    "            filtered_text = re.sub(pattern_urls, preserve_urls, text)\n",
    "            filtered_text = re.sub(pattern_emails, preserve_emails, filtered_text)\n",
    "            processed_text = remove_extra_white_space_and_punctuations(filtered_text)\n",
    "\n",
    "            return processed_text\n",
    "\n",
    "        tokens = word_tokenize(process_text(text))\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.lower() not in stop_words]\n",
    "        processed_text = ' '.join(processed_tokens)\n",
    "\n",
    "        return processed_text\n",
    "\n",
    "    text = convert_file_to_text(file_path)\n",
    "    processed_text = clean_text(text)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "def update_resume_data(resume_data, resume_name, data):\n",
    "    for item in resume_data:\n",
    "        if resume_name in item:\n",
    "            item[resume_name] = data\n",
    "            break\n",
    "    else:\n",
    "        resume_data.append({resume_name: data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_resumes_in_file(file_path, json_filename):\n",
    "    resume_data = []\n",
    "    \n",
    "    # Extract the resume name from the file path\n",
    "    resume_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    retry_count = 0\n",
    "    max_retries = 3\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            # Clean the resume text\n",
    "            cleaned_text = Token_Reduction(file_path)\n",
    "\n",
    "            # Pass cleaned text to OpenAI Chat API\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": f'''You are an <accurate resume parser>.  \\           \n",
    "                                    You extract right values from resume text for only \\\n",
    "                                    \"Name\": , \"Email\": , \"Contact_Number\": , \"Experience_years\": , \"Skills\": [ ], \"Companies\": [ ], \"Designation\": , \"Certifications\": [ ], \"Publications\": [ ], \"Social_Links\": [ ]\n",
    "                                    Give output in json format.\n",
    "                                    Resume text: ```{cleaned_text}``` '''\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Extract response from OpenAI API\n",
    "            data = response['choices'][0]['message']['content']\n",
    "\n",
    "            # Save the resume data along with the resume name\n",
    "            update_resume_data(resume_data, resume_name, data)\n",
    "\n",
    "            # Save updated resume data to the JSON file\n",
    "            output_file = json_filename + '.json'\n",
    "            with open(output_file, 'w') as json_file:\n",
    "                json.dump(resume_data, json_file, indent=4)\n",
    "\n",
    "            break  # Break out of the retry loop if processing is successful\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file: {file_path}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "            retry_count += 1\n",
    "            print(f\"Retrying... Attempt {retry_count}/{max_retries}\")\n",
    "\n",
    "    return resume_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'CKMourya_Resume': '{\\n  \"Name\": \"Chinta Krishna Mourya\",\\n  \"Email\": \"mouryachinta19@gmail.com\",\\n  \"Contact_Number\": \"+91-7793981667\",\\n  \"Experience_years\": \"2\",\\n  \"Skills\": [\\n    \"Machine Learning\",\\n    \"Natural Language Processing\",\\n    \"Python\",\\n    \"Neural Network\"\\n  ],\\n  \"Companies\": [\\n    \"IIT Kharagpur\",\\n    \"INEURON.AI\",\\n    \"BTechinOceanEngineering\",\\n    \"WestBengal\",\\n    \"BIITJR.COLLEGE\",\\n    \"MAY2021WestBengal\",\\n    \"AndhraPradesh\",\\n    \"MPC\",\\n    \"RHIGH-SCHOOL\",\\n    \"BLUEAI-LABS\",\\n    \"Apr2023-May2023\"\\n  ],\\n  \"Designation\": [\\n    \"Data Science Intern\",\\n    \"Python Programmer\",\\n    \"AI Engineer\"\\n  ],\\n  \"Certifications\": [\\n    \"Udemy-Machine Learning A-Z Python\",\\n    \"iNeuron-Full Stack Data Science Bootcamp\"\\n  ],\\n  \"Publications\": [],\\n  \"Social_Links\": [\\n    \"http://www.linkedin.com/in/chinta-krishna-mourya-949aab178/\",\\n    \"http://github.com/ChintaKrishnaMourya\"\\n  ]\\n}'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_resumes_in_file(r\"C:\\Users\\Mourya\\Desktop\\CKMourya_Resume.pdf\", \"mourya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Resume Name         | Name                   | Email                  | Contact Number   | Experience (Years)   | Skills                                                                                                                                                                                                                                                                                                                                                                                          | Companies                                                                                                                                                                                                                                                                                                                                                                   | Designation                | Certifications                                                                                                                                                  | Publications   | Social Links                                                                                       |\n",
      "|:--------------------|:-----------------------|:-----------------------|:-----------------|:---------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------|:---------------------------------------------------------------------------------------------------|\n",
      "| Amal Resume_DS.docx | Amal Jose              | jose.amajose@gmail.com | 7076607504       | 2+                   | Python, SQL, Numpy, Pandas, PuLps, scipy, spacy, Tensorflow, Keras, Scikit, Pyomo, CC++, Java, MATLAB, OperationsResearch, MachineLearning, AdvancedCalculus, Calculus, NetworkAnalysis                                                                                                                                                                                                         | BlueAI-Labs-AIConsultingServices, ExerciseandStressIntensityanalysisusingECGUniversityHertfordshire, ClimateConnectDigital, ExxonMobilCorporationBTCInspectionOptimizationEngineer, PROJECTSEVCharginginfrastructureOptimizationShellAIHackathon, ComputationalanalysisofdragonunderwatervehicleMasterThesis, Non-linearStructuralAnalysisofCorrodedPipelinesBachelorThesis | Data Scientist             | PythonforAI-IBMCertificationIntroductiontoSQL-DatacampMachineLearning, Specialization-StanfordOnlineDSA-UdacityStatistics-LinkedInLearning                      |                |                                                                                                    |\n",
      "| CKMourya_Resume     | Chinta Krishna Mourya  | mouryajes6@gmail.com   | +91-7793981667   | 2                    | Machine Learning, Natural Language Processing, Python, Neural Network, Statistical Modeling, SQL, Flask                                                                                                                                                                                                                                                                                         | IIT Kharagpur, BTechinOceanEngineering, BIITJR.COLLEGE, BHASHYAMHIGHSCHOOL                                                                                                                                                                                                                                                                                                  | Data Science Intern        | Udemy-Machine Learning A-Z Python, iNeuron-Full Stack Data Science Bootcamp                                                                                     |                | http://www.linkedin.com/in/chinta-krishna-mourya-949aab178/, http://github.com/ChintaKrishnaMourya |\n",
      "| Gayas Ummer P P     | GAYAS UMMER P P        | gayasummerpp@gmail.com | +91 8593930651   | 4                    | AI, machine learning, problem-solving, programming, creative thinking, quantitative modelling                                                                                                                                                                                                                                                                                                   | DataMites, Excellence SEO SEM SMM Email Academic Assist, Keltron Aroor, TechByHeart Kochi, Airport Authority India, ISTP Kozhikode                                                                                                                                                                                                                                          | Data Scientist AI Engineer | IABAC Certified Artificial Intelligence Engineer, JAINX Certified Artificial Intelligence Engineer, Arduino Certified Artificial Intelligence, NASSCOM Engineer |                |                                                                                                    |\n",
      "| Koushik_RESUME      | CHINTA KRISHNA KOUSHIK |                        |                  |                      | Probability statistic stochastic process, Introduction Programming, Linear Algebra Engineers, Computational Techniques, Numerical Tech Engineers, Biostatistics, Computational Methods, Materials Engr Series matrix, Linear Algebra Engineers, Introduction Electrochemical Impedance Spectroscopy, Programming language, Libraries Python C, Django NumPy HTML CSS JS, MATLAB Pytorch Sklearn |                                                                                                                                                                                                                                                                                                                                                                             |                            |                                                                                                                                                                 |                |                                                                                                    |\n",
      "| Sudheendra          | SUDHEENDRA RAO K       | sudhee1997@gmail.com   | +91 8971934066   | 2+                   | Supervised Learning, NLP, Data Visualization, Time Series                                                                                                                                                                                                                                                                                                                                       | FruitPunch AI Netherlands, Spartificial, ExxonMobil Services Technology Pvt. Ltd, Lafabrica Craft Pvt. Ltd, ExxonMobil Services Technology Pvt. Ltd, Indian Institute Technology-Bombay                                                                                                                                                                                     | AI Engineer                | Advanced Certificate Programme Data Science IIIT-B, Python Statistics Financial Analysis HKU Coursera                                                           |                | http://www.linkedin.com/in/sudheendra-rao-k                                                        |\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load the JSON data from the file\n",
    "with open(\"5_api_data.json\") as json_file:\n",
    "    resume_data = json.load(json_file)\n",
    "\n",
    "# Extract the resume details and create a list of rows\n",
    "table_rows = []\n",
    "for resume in resume_data:\n",
    "    resume_name = list(resume.keys())[0]\n",
    "    resume_details = json.loads(resume[resume_name])\n",
    "    name = resume_details.get('Name', '')\n",
    "    email = resume_details.get('Email', '')\n",
    "    contact_number = resume_details.get('Contact_Number', '')\n",
    "    experience_years = resume_details.get('Experience_years', '')\n",
    "    skills = ', '.join(resume_details.get('Skills', []))\n",
    "    companies = ', '.join(resume_details.get('Companies', []))\n",
    "    designation = resume_details.get('Designation', '')\n",
    "    certifications = ', '.join(resume_details.get('Certifications', []))\n",
    "    publications = ', '.join(resume_details.get('Publications', []))\n",
    "    social_links = ', '.join(resume_details.get('Social_Links', []))\n",
    "\n",
    "    table_rows.append([\n",
    "        resume_name, name, email, contact_number, experience_years, skills, companies,\n",
    "        designation, certifications, publications, social_links\n",
    "    ])\n",
    "\n",
    "# Display the table\n",
    "table_headers = [\n",
    "    \"Resume Name\", \"Name\", \"Email\", \"Contact Number\", \"Experience (Years)\", \"Skills\",\n",
    "    \"Companies\", \"Designation\", \"Certifications\", \"Publications\", \"Social Links\"\n",
    "]\n",
    "table = tabulate(table_rows, headers=table_headers, tablefmt=\"pipe\")\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
